# Spark RFM 聚类专项升级进展文档

> 更新时间: 2026-02-25 01:05

## 升级目标

对 `spark_final.py` 中 RFM 聚类与打标逻辑进行四大专项升级。

---

## 改造清单与完成状态

### ✅ 1. 可配置 R/F/M 权重

**问题**：等权重打分 `score = -c[0] + c[1] + c[2]` 无法适配不同业务线。

**方案**：在 CONFIG 中新增 `rfm_weights` 配置：
```python
"rfm_weights": {"R": -0.2, "F": 0.3, "M": 0.5}
```
加权求和：`weighted_score = w_R * R + w_F * F + w_M * M`

---

### ✅ 2. 智能化标签映射（基于特征判定）

**问题**：按分数排名强行套标签，脱离业务含义。

**方案**：引入 `classify_cluster()` 函数，基于标准化后聚类中心特征值判定：

| 优先级 | 判定规则 | 标签 |
|--------|----------|------|
| 1 | R > 0.5（高流失风险） | 流失/沉睡客户 |
| 2 | M > 0.3 且 R < 0（高消费且近期活跃） | 核心高价值客户 |
| 3 | F > 0.3 且 M ≤ 0.3（高频低客单价） | 高频忠诚客户 |
| 4 | R ≤ 0 且 F/M 至少一项正向 | 潜力发展客户 |
| 5 | 兜底 | 一般维持客户 |

阈值通过 `CONFIG["rfm_thresholds"]` 可配置。

---

### ✅ 3. 模型持久化预留

预留 `KMeansModel.save()` + `StandardScalerModel.save()` 逻辑，通过 `CONFIG["model_save_path"]` 控制：
- 设置路径 → 自动保存模型
- 不设置 → 跳过，附注释说明在线加载方式

---

### ✅ 4. 聚类解释性输出

`DataQualityReport` 新增 `add_cluster_profile()` 方法，管道结束时输出各簇原始 RFM 均值：
```
🏷️ 聚类画像 (Cluster Profiles):
  Cluster 0 [核心高价值客户] (126人): R=2.31, F=15.70, M=892.45
  Cluster 1 [高频忠诚客户]   (342人): R=5.12, F=11.30, M=234.18
  Cluster 2 [流失/沉睡客户]  (891人): R=28.50, F=1.20, M=45.60
```

---

## 验证结果

- ✅ Python 语法检查通过 (`py_compile`)

## 文件变更

| 文件 | 操作 | 说明 |
|------|------|------|
| `spark_final.py` | 修改 | RFM 四项专项升级 |
| `doc/spark_rfm_upgrade_progress.md` | 新建 | 本文档 |
