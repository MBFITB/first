"""
æ•°æ®åŠ è½½å™¨
è´Ÿè´£ SparkSession åˆå§‹åŒ–ã€CSV æ•°æ®è¯»å–ã€æ¸…æ´—å’Œå¤§å®½è¡¨æ„å»ºã€‚
"""

import os
import sys

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark import StorageLevel


# ç¡®ä¿ PySpark Worker ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨
os.environ['PYSPARK_PYTHON'] = sys.executable


class DataLoader:
    """Spark åˆå§‹åŒ– + æ•°æ®åŠ è½½ + æ¸…æ´—"""

    def __init__(self, config, dq):
        """
        :param config: ConfigManager å®ä¾‹
        :param dq: DataQualityReport å®ä¾‹
        """
        self.config = config
        self.dq = dq
        self.spark = None
        self.df_joined = None       # æ¸…æ´—åçš„å¤§å®½è¡¨ï¼ˆå·²ç¼“å­˜ï¼‰
        self.max_date_str = None    # æ•°æ®é›†æˆªæ­¢æ—¥æœŸ

    def init_spark(self) -> SparkSession:
        """åˆ›å»º SparkSession"""
        cfg = self.config
        self.spark = SparkSession.builder \
            .appName("Strict_DS_Pipeline") \
            .config("spark.driver.memory", cfg["driver_memory"]) \
            .config("spark.ui.enabled", "false") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.default.parallelism", cfg["default_parallelism"]) \
            .config("spark.locality.wait", cfg["locality_wait"]) \
            .config("spark.sql.shuffle.partitions", cfg["default_parallelism"]) \
            .config("spark.jars.packages",
                    "com.clickhouse:clickhouse-jdbc:0.6.0,"
                    "org.apache.httpcomponents.client5:httpclient5:5.3.1") \
            .getOrCreate()
        return self.spark

    def load_and_clean(self):
        """
        åŠ è½½ CSVã€é¢„å¤„ç†ã€æ„å»ºå¤§å®½è¡¨å¹¶ç¼“å­˜ã€‚
        è¿”å› (df_joined, max_date_str)
        """
        print("ğŸš€ [1/6] åŠ è½½æ•°æ®ä¸é¢„å¤„ç†...")
        cfg = self.config
        spark = self.spark

        # è¯»å–åŸå§‹æ•°æ®
        schema_bhv = "user_id INT, item_id INT, category_id INT, type STRING, ts INT"
        df_bhv_raw = spark.read.csv(cfg["behavior_csv"], header=False, schema=schema_bhv)
        df_bhv = df_bhv_raw.limit(5000000) if cfg["data_limit"] else df_bhv_raw

        df_items = spark.read.csv(cfg["items_csv"], header=True, inferSchema=True)
        df_users_info = spark.read.csv(cfg["users_csv"], header=True, inferSchema=True)

        # è®°å½•åŸå§‹æ•°æ®é‡
        raw_count = df_bhv.count()
        self.dq.add_metric("åŸå§‹è¡Œä¸ºæ•°æ®è¡Œæ•°", raw_count)
        self.dq.add_metric("å•†å“ç»´è¡¨è¡Œæ•°", df_items.count())
        self.dq.add_metric("ç”¨æˆ·ç»´è¡¨è¡Œæ•°", df_users_info.count())

        # æ—¥æœŸè§£æä¸è¿‡æ»¤
        df_cleaned = df_bhv \
            .withColumn("date", F.to_date(F.from_unixtime(F.col("ts")))) \
            .filter((F.col("date") >= "2017-11-01") & (F.col("date") <= "2017-12-10")) \
            .withColumn("order_id", F.concat_ws('_', F.col('user_id'), F.col('ts'), F.col('item_id')))

        max_date_val = df_cleaned.agg(F.max("date")).collect()[0][0]
        self.max_date_str = str(max_date_val)
        print(f"  [*] æ•°æ®é›†ä¸šåŠ¡æˆªæ­¢æ—¥æœŸ: {self.max_date_str}")
        self.dq.add_metric("æ•°æ®é›†æˆªæ­¢æ—¥æœŸ", self.max_date_str)

        # æ„å»ºå¤§å®½è¡¨ï¼ˆJOIN ç»´è¡¨ï¼‰
        df_with_price = df_cleaned \
            .join(df_items.drop("category_id"), on="item_id", how="left") \
            .join(df_users_info, on="user_id", how="left")

        # æ•°æ®è´¨é‡æ ¡éªŒ
        missing_price_count = df_with_price.filter(F.col("price").isNull()).count()
        missing_channel = df_with_price.filter(F.col("channel").isNull()).count()
        missing_age = df_with_price.filter(F.col("age_group").isNull()).count()

        self.dq.add_metric("ç¼ºå¤±ä»·æ ¼è®°å½•æ•°", missing_price_count)
        self.dq.add_metric("ç¼ºå¤±æ¸ é“è®°å½•æ•°", missing_channel)
        self.dq.add_metric("ç¼ºå¤±å¹´é¾„åˆ†ç»„è®°å½•æ•°", missing_age)

        if missing_price_count > 0:
            self.dq.add_warning(
                f"å‘ç° {missing_price_count} æ¡è®°å½•ç¼ºå¤±ä»·æ ¼(price)å­—æ®µï¼Œå·²è¢«ä¸¢å¼ƒã€‚"
                f"ç”Ÿäº§ç¯å¢ƒä¸‹åº”å†™å…¥æ­»ä¿¡é˜Ÿåˆ—æ’æŸ¥ã€‚"
            )
            print(f"  âš ï¸ [æ•°æ®è´¨é‡å‘Šè­¦] {missing_price_count} æ¡ç¼ºå¤±ä»·æ ¼è®°å½•å·²ä¸¢å¼ƒ")
        if missing_channel > 0:
            self.dq.add_warning(f"å‘ç° {missing_channel} æ¡è®°å½•ç¼ºå¤±æ¸ é“(channel)å­—æ®µï¼Œå·²å¡«å……ä¸º'æœªçŸ¥æ¸ é“'")
        if missing_age > 0:
            self.dq.add_warning(f"å‘ç° {missing_age} æ¡è®°å½•ç¼ºå¤±å¹´é¾„åˆ†ç»„(age_group)å­—æ®µï¼Œå·²å¡«å……ä¸º'æœªçŸ¥'")

        # æ¸…æ´—å¹¶ç¼“å­˜ï¼ˆMEMORY_AND_DISK ç­–ç•¥ï¼šä¼˜å…ˆå†…å­˜ï¼Œæº¢å‡ºå†™ç£ç›˜ï¼‰
        self.df_joined = df_with_price \
            .dropna(subset=["price"]) \
            .fillna({"channel": "æœªçŸ¥æ¸ é“", "age_group": "æœªçŸ¥"}) \
            .persist(StorageLevel.MEMORY_AND_DISK)

        clean_count = self.df_joined.count()
        self.dq.add_metric("æ¸…æ´—åæœ‰æ•ˆè®°å½•æ•°", clean_count)
        self.dq.add_metric("æ•°æ®ä¸¢å¼ƒç‡", f"{(1 - clean_count / max(raw_count, 1)) * 100:.2f}%")
        print(f"  [*] æ¸…æ´—åæœ‰æ•ˆè®°å½•æ•°: {clean_count:,}")

        return self.df_joined, self.max_date_str

    def unpersist(self):
        """é‡Šæ”¾ç¼“å­˜"""
        if self.df_joined is not None:
            self.df_joined.unpersist()

    def stop_spark(self):
        """åœæ­¢ SparkSession"""
        if self.spark is not None:
            self.spark.stop()
