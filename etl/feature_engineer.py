"""
ç‰¹å¾å·¥ç¨‹æ¨¡å—
è´Ÿè´£ RFM æŒ‡æ ‡è®¡ç®—ã€StandardScaler æ ‡å‡†åŒ–ã€KMeans èšç±»å¯»ä¼˜å’Œæ™ºèƒ½æ ‡ç­¾æ˜ å°„ã€‚
"""

import functools

from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator


class FeatureEngineer:
    """RFM ç‰¹å¾å·¥ç¨‹ + KMeans èšç±»æ‰“æ ‡"""

    def __init__(self, config, dq):
        self.config = config
        self.dq = dq

    def run(self, df_joined, max_date_str):
        """
        æ‰§è¡Œå®Œæ•´çš„ RFM èšç±»æµç¨‹ã€‚
        è¿”å› user_rfm DataFrameï¼ˆåŒ…å« user_id å’Œ rfm_label ä¸¤åˆ—ï¼‰ã€‚
        """
        print("ğŸ§ª [2/6] ç‰¹å¾å·¥ç¨‹ä¸ KMeans èšç±»æ‰“æ ‡...")

        # Step 1: è®¡ç®— RFM åŸå§‹æŒ‡æ ‡
        rfm_base = df_joined.filter(F.col("type") == "buy").groupBy("user_id").agg(
            F.datediff(F.lit(max_date_str), F.max("date")).alias("R"),
            F.countDistinct("order_id").alias("F"),
            F.sum("price").alias("M")
        )

        # Step 2: VectorAssembler ç»„è£…ç‰¹å¾å‘é‡
        assembler = VectorAssembler(inputCols=["R", "F", "M"], outputCol="features_raw")
        rfm_vec = assembler.transform(rfm_base)

        # Step 3: StandardScaler æ ‡å‡†åŒ–ï¼ˆæ¶ˆé™¤é‡çº²å·®å¼‚ï¼‰
        scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=True)
        scaler_model = scaler.fit(rfm_vec)
        rfm_scaled = scaler_model.transform(rfm_vec)

        # Step 4: KMeans è½®å»“ç³»æ•°è‡ªåŠ¨é€‰ K
        best_k, best_score, best_model = self._auto_kmeans(rfm_scaled)
        rfm_clustered = best_model.transform(rfm_scaled)

        # Step 5: æ¨¡å‹æŒä¹…åŒ–ï¼ˆå¯é€‰ï¼‰
        self._persist_model(best_model, scaler_model)

        # Step 6: æ™ºèƒ½æ ‡ç­¾æ˜ å°„
        cluster_to_label = self._build_label_mapping(best_model)

        # Step 7: æ”¶é›†èšç±»ç”»åƒåˆ° DQ æŠ¥å‘Š
        self._collect_cluster_profiles(rfm_clustered, cluster_to_label)

        # Step 8: ç”Ÿæˆæœ€ç»ˆ user_rfm
        mapping_expr = functools.reduce(
            lambda acc, kv: acc.when(F.col("cluster") == kv[0], kv[1]),
            cluster_to_label.items(),
            F.when(F.lit(False), "")
        ).otherwise("æœªçŸ¥ç¾¤ä½“")

        user_rfm = rfm_clustered.withColumn("rfm_label", mapping_expr).select("user_id", "rfm_label")
        return user_rfm

    def _auto_kmeans(self, rfm_scaled):
        """è½®å»“ç³»æ•°è‡ªåŠ¨é€‰ Kï¼ˆK=3~5ï¼‰"""
        evaluator = ClusteringEvaluator(
            predictionCol="cluster", featuresCol="features",
            metricName="silhouette", distanceMeasure="squaredEuclidean"
        )

        best_k, best_score, best_model = 4, -1, None
        for k in range(3, 6):
            kmeans = KMeans(k=k, seed=42, featuresCol="features", predictionCol="cluster")
            model = kmeans.fit(rfm_scaled)
            predictions = model.transform(rfm_scaled)
            score = evaluator.evaluate(predictions)
            if score > best_score:
                best_k, best_score, best_model = k, score, model

        print(f"  [KMeans] è‡ªåŠ¨å¯»ä¼˜å®Œæˆï¼Œæœ€ä¼˜K: {best_k} (Silhouette: {best_score:.4f})")
        self.dq.add_metric("KMeans æœ€ä¼˜Kå€¼", best_k)
        self.dq.add_metric("KMeans è½®å»“ç³»æ•°", f"{best_score:.4f}")
        return best_k, best_score, best_model

    def _persist_model(self, best_model, scaler_model):
        """ç”Ÿäº§ç¯å¢ƒæ¨¡å‹æŒä¹…åŒ–"""
        model_path = self.config.get("model_save_path")
        if model_path:
            best_model.save(model_path)
            scaler_model.save(model_path + "_scaler")
            print(f"  [æ¨¡å‹æŒä¹…åŒ–] KMeans æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
            print(f"  [æ¨¡å‹æŒä¹…åŒ–] Scaler æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}_scaler")

    def _build_label_mapping(self, best_model) -> dict:
        """åŸºäºèšç±»ä¸­å¿ƒç‰¹å¾çš„æ™ºèƒ½æ ‡ç­¾åˆ¤å®š"""
        centers = best_model.clusterCenters()
        weights = self.config["rfm_weights"]
        thresholds = self.config["rfm_thresholds"]

        cluster_to_label = {}
        for i, c in enumerate(centers):
            weighted_score = weights["R"] * c[0] + weights["F"] * c[1] + weights["M"] * c[2]
            label = self._classify_cluster(c, thresholds)
            cluster_to_label[i] = label
            print(f"  [KMeans] Cluster {i}: R={c[0]:.3f}, F={c[1]:.3f}, M={c[2]:.3f} "
                  f"â†’ åŠ æƒåˆ†={weighted_score:.3f} â†’ æ ‡ç­¾='{label}'")

        print(f"  [KMeans] æœ€ç»ˆæ˜ å°„: {cluster_to_label}")
        return cluster_to_label

    @staticmethod
    def _classify_cluster(center, th):
        """
        åŸºäºèšç±»ä¸­å¿ƒæ ‡å‡†åŒ–ç‰¹å¾å€¼è¿›è¡Œæ™ºèƒ½æ ‡ç­¾åˆ¤å®šã€‚
        åˆ¤å®šä¼˜å…ˆçº§ï¼šæµå¤±æ£€æµ‹ > é«˜ä»·å€¼è¯†åˆ« > é«˜é¢‘è¯†åˆ« > æ½œåŠ›è¯†åˆ« > ä¸€èˆ¬
        """
        r_val, f_val, m_val = center[0], center[1], center[2]

        if r_val > th["high_r"]:
            return "æµå¤±/æ²‰ç¡å®¢æˆ·"
        if m_val > th["high_m"] and r_val < 0:
            return "æ ¸å¿ƒé«˜ä»·å€¼å®¢æˆ·"
        if f_val > th["high_f"] and m_val <= th["high_m"]:
            return "é«˜é¢‘å¿ è¯šå®¢æˆ·"
        if r_val <= 0 and (f_val > 0 or m_val > 0):
            return "æ½œåŠ›å‘å±•å®¢æˆ·"
        return "ä¸€èˆ¬ç»´æŒå®¢æˆ·"

    def _collect_cluster_profiles(self, rfm_clustered, cluster_to_label):
        """æ”¶é›†å„èšç±»ç°‡çš„åŸå§‹ RFM å‡å€¼ï¼Œå†™å…¥ DQ æŠ¥å‘Š"""
        cluster_stats = rfm_clustered.groupBy("cluster").agg(
            F.round(F.avg("R"), 2).alias("R_mean"),
            F.round(F.avg("F"), 2).alias("F_mean"),
            F.round(F.avg("M"), 2).alias("M_mean"),
            F.count("user_id").alias("user_count"),
        ).collect()

        for row in cluster_stats:
            cid = row["cluster"]
            self.dq.add_cluster_profile(
                cluster_id=cid,
                label=cluster_to_label.get(cid, "æœªçŸ¥"),
                r_mean=row["R_mean"],
                f_mean=row["F_mean"],
                m_mean=row["M_mean"],
                user_count=row["user_count"],
            )
